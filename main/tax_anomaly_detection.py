# -*- coding: utf-8 -*-
"""Tax Anomaly Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xyX47yK_hi_ur5wlE-wmDlTH4sF1lgbf

# **ANOMALY DETECTION ALGORITHM ON ECONOMIC AND TAX DATA**

# **Introduction**

Economic and tax data often contain irregularities that may not fit expected patterns. These irregularities, or anomalies, can signal issues ranging from reporting errors to unusual market activity, and in some cases, they may align with politically sensitive periods such as elections. For policymakers, auditors, and researchers, being able to detect and understand these anomalies is crucial for accurate interpretation of economic performance.

This project applies advanced statistical and machine learning methods to identify anomalies in Nigeria’s fiscal and economic indicators. The dataset includes variables such as Petroleum Profits Tax, VAT, Brent Crude Oil Price, and different sectors of GDP. Since raw economic data can be skewed or highly correlated, preprocessing steps such as transformations, dimensionality reduction, and multicollinearity checks were necessary to prepare the variables for robust analysis.

To ensure reliability, multiple anomaly detection techniques were applied, including Isolation Forest, Local Outlier Factor (LOF), KNN-based detection, HBOS, and Elliptic Envelope. Because each model has different strengths and weaknesses, their results were combined using consensus voting. Finally, anomalies were evaluated against election flags to check whether unusual economic behavior coincided with election periods.

By combining rigorous statistical preprocessing with multiple anomaly detection approaches, this project provides a balanced, evidence-based framework for analyzing irregularities in economic and tax data. The results offer both methodological insights and practical relevance for monitoring fiscal behavior in Nigeria.

# **Project Aim and Objectives**

**Aim:**
The aim of this project is to detect and analyze anomalies in Nigeria’s fiscal and economic indicators, with particular attention to how these anomalies align with major political events such as elections.

**Objectives:**

* To clean and prepare macroeconomic and tax datasets for reliable analysis.
* To reduce skewness and multicollinearity in the variables using transformations, correlation checks, and PCA.
* To apply multiple anomaly detection models and compare their performance across metrics such as Average Precision, PR AUC, and Precision\@k.
* To combine results from different models through consensus voting for more stable anomaly detection.
* To evaluate whether detected anomalies show patterns around election periods, supporting interpretation of unusual economic behavior.
* To visualize anomalies in both economic space (scatter plots) and over time (time series plots).

#### **Install Dependencies**
"""

!pip install pyod

# Standard imports
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.model_selection import ParameterGrid
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import kendalltau
from sklearn.metrics import average_precision_score, precision_recall_curve, auc
from pyod.models.hbos import HBOS
from pyod.models.knn import KNN
from sklearn.decomposition import PCA
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# File path (adjust this to your actual folder and file name)
file_path = '/content/drive/MyDrive/FIRS Taxes.xlsx'

# Load Excel file
df = pd.read_excel(file_path)

# Display first rows
df.head()

"""#### **Data Cleaning and Imputation**

Before performing anomaly detection, it was necessary to clean the dataset to ensure accuracy and consistency. This step focused on removing irrelevant information, handling missing values, and preparing the data for analysis.

**Actions Taken:**

*   **Dropped Irrelevant Columns:** Columns such as Company Income Tax, Gas Income, and Government Spending were removed because they were not relevant to the anomaly detection task. This helps reduce noise and potential redundancy.

*   **Replaced Zeros with Missing Values:** In numeric columns, zeros were treated as missing values (NaN). This ensures that zero values, which could represent unrecorded data rather than true zeros, do not distort the analysis.

*   **Created Year_Quarter Identifier:** A combined Year_Quarter column was generated to allow grouping of data by period. This enables imputation of missing values based on quarterly trends, preserving temporal patterns.

*   **Median Imputation by Quarter:** Missing numeric values were filled using the median within each Year_Quarter group. The median is robust to outliers and ensures that extreme values do not bias the imputation.

*   **Fallback Forward/Backward Fill:** Any remaining missing values were filled by carrying the last known value forward or the next known value backward. This guarantees a complete dataset without introducing strong assumptions.

"""

# Drop irrelevant columns
df_clean = df.drop(columns=[
    "Company Income Tax",
    "Gas Income",
    "Government Spending (N' Billion)"
], errors="ignore")

# Replace 0s with NaN (only in numeric columns)
df_clean[df_clean.select_dtypes(include=[np.number]).columns] = \
    df_clean.select_dtypes(include=[np.number]).replace(0, np.nan)

# Create Year_Quarter for grouping
df_clean["Year_Quarter"] = df_clean["Year"].astype(str) + "_" + df_clean["Quarter"]

# Apply median imputation only to numeric columns
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns

df_clean[numeric_cols] = df_clean.groupby("Year_Quarter")[numeric_cols].transform(
    lambda x: x.fillna(x.median())
)

# Fallback: forward/backward fill if still missing
df_clean[numeric_cols] = df_clean[numeric_cols].fillna(method="ffill").fillna(method="bfill")

# Check missing values after treatment
missing_report = df_clean.isnull().sum()

print("Columns after cleaning:")
print(df_clean.columns.tolist())
print("\nMissing values report after treatment:")
print(missing_report)

df_clean.head()

# Drop Election Flag column
df_desc = df_clean.drop(columns=["Election Flag", "Year_Quarter", "USD/NGN", "Capital Gains Tax", "Stamp Duty"], errors="ignore")

# Descriptive statistics
descriptives = df_desc.describe().T

descriptives

"""#### **Exploratory Data Analysis and Skewness Assessment**

**Purpose:**
Before running anomaly detection, it was important to understand the relationships, trends, and distributions of the variables. This step helped identify correlations, temporal patterns, and potential outliers that could influence model performance. It also assessed whether transformations were needed to stabilise distributions.

**Actions Taken:**

* **Correlation Heatmap:** A half-matrix heatmap was generated for numeric tax and economic variables. Redundant or irrelevant variables (USD/NGN, Capital Gains Tax, Stamp Duty) were excluded to focus on meaningful relationships. This helps identify highly correlated variables and informs which features may need careful handling in anomaly detection.

* **Line Plots for Key Revenue Sources:** Trends of Petroleum Profits Tax and VAT over time were plotted by year and quarter. Visualising temporal patterns highlights seasonal or periodic spikes and trends in revenue collection.

* **Sectoral GDP Trends:** Line plots for Industry, Manufacturing, Trade, ICT, and Real Estate GDP sectors were created to examine sector-specific economic activity over time. This contextualises tax revenues against broader economic trends.

* **Boxplots for Taxes and GDP Sectors:** Boxplots were used to visualise extreme values and distribution shapes for both taxes and GDP variables. This helps detect outliers and understand variable dispersion, which informs preprocessing and potential transformation steps.

* **Skewness Calculation:** Skewness of numeric variables was calculated to assess the symmetry of distributions. Variables with |skew| > 1 are considered highly skewed and candidates for log transformation, while those between 0.5 and 1 are moderately skewed. Symmetric variables (|skew| ≤ 0.5) do not require transformation.
"""

# Make a copy without non-numeric columns
df_viz = df_clean.drop(columns=["Election Flag", "Year", "Quarter", "Year_Quarter", "USD/NGN"], errors="ignore")


# 1. Correlation heatmap (half matrix)
# Exclude redundant variables
exclude_vars = ["USD/NGN", "Capital Gains Tax", "Stamp Duty"]
df_corr = df_viz.drop(columns=exclude_vars, errors="ignore")
corr = df_corr.corr(numeric_only=True)
mask = np.triu(np.ones_like(corr, dtype=bool))
plt.figure(figsize=(12,8))
sns.heatmap(corr, mask=mask, annot=True, cmap="coolwarm", fmt=".2f",
            linewidths=.5, cbar_kws={"shrink": 0.8})
plt.title("Correlation Heatmap of Tax and Economic Variables (Lower Triangle)")
plt.show()

# 2. Line plots for key revenue sources
plt.figure(figsize=(12,6))
for col in ["Petroleum Profits Tax", "VAT"]:
    plt.plot(df_clean["Year"].astype(str) + "-" + df_clean["Quarter"], df_clean[col], label=col)
plt.xticks(rotation=90)
plt.title("Trends in Tax Revenues Over Time")
plt.legend()
plt.tight_layout()
plt.show()

# 3. GDP sectors over time
plt.figure(figsize=(12,6))
for col in ["Nominal GDP (Industry)", "Nominal GDP (Manufacturing)", "Nominal GDP (Trade)",
            "Nominal GDP (Information and Communication)", "Nominal GDP (Real Estate)"]:
    plt.plot(df_clean["Year"].astype(str) + "-" + df_clean["Quarter"], df_clean[col], label=col)
plt.xticks(rotation=90)
plt.title("Sectoral GDP Trends Over Time")
plt.legend()
plt.tight_layout()
plt.show()

# Define groups
tax_cols = ["Petroleum Profits Tax", "VAT"]
gdp_cols = ["Nominal GDP (Crude Petroleum and Gas)", "Nominal GDP (Industry)",
            "Nominal GDP (Manufacturing)", "Nominal GDP (Trade)",
            "Nominal GDP (Information and Communication)", "Nominal GDP (Real Estate)"]

# Boxplot for Taxes
plt.figure(figsize=(10,6))
sns.boxplot(data=df_clean[tax_cols])
plt.title("Distribution and Outliers of Tax Revenues")
plt.xticks(rotation=45)
plt.show()

# Boxplot for GDP Sectors
plt.figure(figsize=(12,6))
sns.boxplot(data=df_clean[gdp_cols])
plt.title("Distribution and Outliers of Sectoral GDPs")
plt.xticks(rotation=45)
plt.show()

# Keep only numeric columns, drop redundant
df_skew = df_viz.drop(columns=["USD/NGN", "Capital Gains Tax", "Stamp Duty"], errors="ignore")

# Calculate skewness
skewness = df_skew.skew(numeric_only=True)

print("Skewness of numeric variables:")
print(skewness)

# Rule of thumb:
# |skew| > 1  → highly skewed (log transform candidate)
# 0.5 < |skew| <= 1 → moderately skewed
# |skew| <= 0.5 → roughly symmetric

"""#### **Skewness Transformation and Multicollinearity Assessment**

**Purpose:**
After initial exploration, some numeric variables were highly skewed. Skewed distributions can distort distance- or density-based anomaly detection models, making it harder to identify unusual points. To stabilise the distributions and improve model performance, a log transformation was applied. Additionally, correlations and multicollinearity were checked to understand feature redundancy and potential effects on anomaly detection.

**Actions Taken:**

* **Identifying Skewed Variables:** Skewness was calculated for all numeric variables. Variables with |skew| > 1, including Petroleum Profits Tax, VAT, Nominal GDP (Information and Communication), and Nominal GDP (Real Estate), were flagged for transformation.

* **Log Transformation:** A log1p (log(x+1)) transformation was applied to the skewed variables. This reduces the influence of extreme values while preserving relative differences, making distributions more symmetric. After transformation, skewness was re-evaluated to confirm improvement.

* **Correlation Analysis:** A correlation heatmap of the transformed numeric variables was generated. Masking the upper triangle clarified pairwise relationships. This helps identify highly correlated variables that may influence anomaly detection methods differently.

* **Variance Inflation Factor (VIF) Calculation:** To quantify multicollinearity, VIF was calculated for all numeric variables including the constant. High VIF values indicate strong linear dependencies among variables. Understanding these relationships informs model interpretation, especially for methods sensitive to redundancy.
"""

# Assuming your dataframe is df_transformed with the transformed variables
numeric_df = df_transformed.select_dtypes(include=['float64', 'int64'])


# 1. Correlation matrix
corr = numeric_df.corr()

# Mask for upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(corr, mask=mask, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5, cbar_kws={"shrink": 0.8})
plt.title("Correlation Heatmap of Transformed Variables (Lower Triangle)")
plt.show()


# --- 2. Variance Inflation Factor (VIF) ---
X = add_constant(numeric_df)  # add intercept for VIF
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i)
                   for i in range(X.shape[1])]

print(vif_data)

"""We started with several GDP variables (Industry, Manufacturing, Trade, ICT, and Real Estate). These variables are often highly correlated with each other, which makes it risky to include them all in a model because of **multicollinearity**.  

To solve this, we applied **Principal Component Analysis (PCA)**. PCA compresses information from many correlated variables into a smaller set of uncorrelated components. In this case, we chose **one component** (`n_components=1`), which created a new variable called the **GDP Composite Index**.  

This index is a weighted combination of the five GDP sectors and captures the largest possible variation across them in a single number. The variance explained (printed out after PCA) tells us how much of the original information is retained by this new index. A higher percentage means the index is a strong summary of the selected GDP variables.  

By doing this, instead of working with five separate but overlapping GDP measures, we now have one clean index that represents overall economic activity from those sectors. This makes analysis simpler and reduces the risk of unstable results due to multicollinearity.
"""

# --- 1. Select GDP variables from df_clean ---
gdp_vars = [
    "Nominal GDP (Industry)",
    "Nominal GDP (Manufacturing)",
    "Nominal GDP (Trade)",
    "Nominal GDP (Information and Communication)",
    "Nominal GDP (Real Estate)"
]

df_gdp = df_clean[gdp_vars].dropna()

# --- 2. PCA to create GDP Composite Index ---
pca = PCA(n_components=1)
df_clean["GDP Composite Index"] = pca.fit_transform(df_gdp)

print(f"Variance explained by GDP Composite Index: {pca.explained_variance_ratio_[0]*100:.2f}%")

# --- 3. Check skewness of new PCA variable ---
skewness = df_clean["GDP Composite Index"].skew()
print("Skewness of GDP Composite Index:", skewness)

# Optional: Apply log or Yeo-Johnson transform if needed
if abs(skewness) > 1:
    df_clean["GDP Composite Index"] = np.log1p(df_clean["GDP Composite Index"] - df_clean["GDP Composite Index"].min() + 1)
    print("Applied log1p transform to GDP Composite Index")
    print("New skewness:", df_clean["GDP Composite Index"].skew())

# --- 4. Select relevant variables for correlation & VIF ---
vars_for_check = [
    "Petroleum Profits Tax",
    "VAT",
    "Brent Crude Oil Price",
    "Nominal GDP (Crude Petroleum and Gas)",
    "GDP Composite Index"
]

df_vif = df_transformed[vars_for_check].dropna()

# --- 5. Correlation matrix (half heatmap) ---
corr = df_vif.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
plt.figure(figsize=(10, 7))
sns.heatmap(corr, mask=mask, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5, cbar_kws={"shrink": 0.8})
plt.title("Correlation Heatmap with GDP Composite Index")
plt.show()

# --- 6. Print unique correlation pairs ---
print("\nCorrelation values (unique pairs):")
for i in range(len(corr.columns)):
    for j in range(i):
        print(f"{corr.columns[i]} vs {corr.columns[j]}: {corr.iloc[i, j]:.3f}")

# --- 7. VIF Calculation ---
X = sm.add_constant(df_vif)
vif_df = pd.DataFrame()
vif_df["Variable"] = X.columns
vif_df["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print("\nVariance Inflation Factor (VIF):")
print(vif_df)

"""#### **Incorporating Election Flag and Feature Scaling**

**Actions Taken:**

* **Election Flag Integration:** Checked that the `Election Flag` column exists in `df_clean` and copied it into `df_transformed`. This binary variable (0 = non-election, 1 = election) captures the presence of election-related effects in the data.

* **Ensuring Numeric Data:** Converted all columns to numeric to prevent errors during model training. Non-numeric values, if any, were coerced to NaN and handled in the next step.

* **Dropping Missing Values:** Any remaining rows with missing values were removed to guarantee complete cases for model input.

* **Feature Separation:** Defined `features` to include tax and economic indicators (VAT, Petroleum Profits Tax, Brent Crude Oil Price, Nominal GDP, GDP Composite Index). Election Flag was kept separate to allow downstream evaluation.

* **Standardisation (Z-score):** Applied `StandardScaler` to numeric features. Scaling ensures that variables with larger magnitudes do not dominate distance-based models like LOF or KNN, and it improves stability for models like Elliptic Envelope.
"""

# Make sure 'election_flag' exists in df_clean
if 'Election Flag' in df_clean.columns:
    df_transformed['Election Flag'] = df_clean['Election Flag'].values
else:
    print("Column 'Election Flag' not found in df_clean")

df_transformed.head()

# Step 1: Data Preparation
print("Columns in df_transformed:", df_transformed.columns.tolist())

# Ensure all numeric
df_transformed = df_transformed.apply(pd.to_numeric, errors='coerce')

# Drop rows with missing values (if any)
df_clean = df_transformed.dropna()

# Features and target separation
features = ['Petroleum Profits Tax', 'VAT', 'Brent Crude Oil Price',
            'Nominal GDP (Crude Petroleum and Gas)', 'GDP Composite Index']

X = df_clean[features].values
election_flag = df_clean['Election Flag'].values  # keep aside for later

# Step 2: Standardisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Scaled dataset shape:", X_scaled.shape)
print("First 5 rows (scaled):\n", X_scaled[:5])

"""#### **Hyperparameter Tuning and Model Execution**

**Purpose:**
Running multiple anomaly detection models with different hyperparameter configurations allows us to identify unusual patterns in tax and economic data while controlling for election effects. Testing parameter grids ensures we capture anomalies without overfitting or being too lenient.

**Actions Taken:**

* **Defined Model Grids:** Five models were chosen: Isolation Forest, LOF, KNN, HBOS, and Elliptic Envelope. For each, a set of key hyperparameters was defined:

  * **Isolation Forest:** Contamination levels of 0.05, 0.1, and 0.15 with 100 trees.
  * **LOF:** Number of neighbors (5, 10, 15) and contamination levels as above.
  * **KNN:** Same neighbor and contamination ranges as LOF.
  * **HBOS:** Bin sizes of 5, 10, 20 with same contamination options.
  * **Elliptic Envelope:** Support fractions 0.8, 0.85, 0.9 with contamination variations.

* **Iterated Through Parameter Grid:** For each model, all combinations of hyperparameters were tested. This approach systematically explores configurations to find balanced settings that detect true anomalies while reducing false positives.

* **Model Execution:**

  * **LOF:** Required `fit_predict` because it evaluates density locally. Anomalies were converted from -1 to 1.
  * **Other Models:** Fit to the scaled data, with anomaly predictions adjusted so that 1 = anomaly. Anomaly scores were extracted using `decision_function` or `predict_proba` when available, providing a continuous measure of “outlierness.”

* **Result Storage:** Predictions and scores for each hyperparameter combination were stored in a dictionary for later comparison and consensus evaluation.
"""

param_grids = {
    "IsolationForest": {
        "model": IsolationForest,  # class
        "params": {"contamination": [0.05, 0.1, 0.15], "n_estimators": [100]}
    },
    "LOF": {
        "model": LocalOutlierFactor,  # class
        "params": {"n_neighbors": [5, 10, 15], "contamination": [0.05, 0.1, 0.15]}
    },
    "KNN": {
        "model": KNN,  # class
        "params": {"n_neighbors": [5, 10, 15], "contamination": [0.05, 0.1, 0.15]}
    },
    "HBOS": {
        "model": HBOS,
        "params": {"n_bins": [5, 10, 20], "contamination": [0.05, 0.1, 0.15]}
    },
    "EllipticEnvelope": {
        "model": EllipticEnvelope,
        "params": {"support_fraction": [0.8, 0.85, 0.9], "contamination": [0.05, 0.1, 0.15]}
    }
}

# Dictionary to store results for all models
results = {}

# Loop through each model and its parameter grid
for model_name, config in param_grids.items():
    model_class = config["model"]
    param_grid = config["params"]

    results[model_name] = []

    for params in ParameterGrid(param_grid):
        print(f"Running {model_name} with params: {params}")

        if model_name == "LOF":
            # LOF requires fit_predict; -1 = anomaly, 1 = normal
            lof = model_class(**params)
            y_pred = lof.fit_predict(X_scaled)
            preds = np.where(y_pred == -1, 1, 0)
            scores = -lof.negative_outlier_factor_

        else:
            mdl = model_class(**params)
            mdl.fit(X_scaled)

            # Some models return -1 for anomalies
            preds = mdl.predict(X_scaled)
            preds = np.where(preds == -1, 1, 0)

            # Determine anomaly score
            if hasattr(mdl, "decision_function"):
                scores = -mdl.decision_function(X_scaled)
            elif hasattr(mdl, "predict_proba"):
                scores = mdl.predict_proba(X_scaled)[:, 1]
            else:
                scores = preds  # fallback if no score function

        results[model_name].append({
            "params": params,
            "preds": preds,
            "scores": scores
        })

print("Step 4 complete: All models executed on scaled features.")

"""#### **Consensus Anomaly Detection and Evaluation**

**Purpose:**
After running multiple anomaly detection models, the next step was to combine their predictions and evaluate their effectiveness. This ensures robustness by reducing false positives and highlights anomalies that consistently appear across different methods. Including the Election Flag allows us to check whether anomalies cluster around election periods.

**Actions Taken:**

* **Aggregated Predictions:** Predictions from all five models (Isolation Forest, LOF, KNN, HBOS, Elliptic Envelope) were collected into a single DataFrame. This makes it easy to compare results and apply consensus rules.

* **Consensus Voting:** An observation was labeled as an anomaly if at least three models agreed. This hybrid approach balances sensitivity and specificity, reducing the impact of any single model producing spurious flags.

* **Election Flag Integration:** The Election Flag was retained alongside predictions to assess whether anomalies correspond with election periods. This contextual check supports interpretation of unusual patterns in tax and economic variables.

* **Evaluation Metrics:**

  * **Average Precision (AP):** Measures how well the model prioritizes true anomalies over normal observations.
  * **PR AUC:** Summarizes model performance across all decision thresholds, especially useful when anomalies are rare.
  * **Precision at k (P\@k):** Indicates the proportion of true anomalies among the top k observations flagged, useful for operational review (k = 8, with sensitivity test k = 4).
  * **Election Period Anomalies:** Counts anomalies occurring during election periods to check if political events influence unusual patterns.

* **Visualizations:**

  * **Scatter Plot:** VAT vs GDP Composite Index, anomalies highlighted. Helps see whether anomalies are concentrated in specific economic conditions.
  * **Time Series Plot:** VAT over time with anomalies marked. Reveals temporal clustering, including election periods.

**Outcome:**
The consensus approach produced a more stable set of anomalies than any single model. Evaluation metrics provide quantitative insight into model performance, and the election flag shows how anomalies correlate with political events. This comprehensive analysis supports both statistical rigor and practical audit relevance.
"""

import pandas as pd
import numpy as np

# Collect predictions from all models into a DataFrame
# Assuming `results` dictionary from Step 4 exists
# We'll use the best hyperparameter configuration for each model (first entry here as example)

pred_df = pd.DataFrame({
    "IsolationForest": results["IsolationForest"][0]["preds"],
    "LOF": results["LOF"][0]["preds"],
    "KNN": results["KNN"][0]["preds"],
    "HBOS": results["HBOS"][0]["preds"],
    "EllipticEnvelope": results["EllipticEnvelope"][0]["preds"]
})

# Consensus voting: label as anomaly if >=3 models agree
pred_df["Consensus_Anomaly"] = (pred_df.sum(axis=1) >= 3).astype(int)

# Add Election Flag for reference
pred_df["Election_Flag"] = election_flag

# Check the number of anomalies
print("Number of anomalies flagged per model:")
for col in pred_df.columns[:-2]:  # exclude Consensus and Election Flag
    print(f"{col}: {pred_df[col].sum()}")

print("\nNumber of anomalies by consensus:", pred_df["Consensus_Anomaly"].sum())

# Optional: preview anomalies alongside election flag
print(pred_df.head(10))

# pred_df from Step 5 contains:
# ['IsolationForest', 'LOF', 'KNN', 'HBOS', 'EllipticEnvelope', 'Consensus_Anomaly', 'Election_Flag']

# Function to calculate P@k
def precision_at_k(y_true, scores, k):
    idx = np.argsort(scores)[::-1]  # descending order
    top_k = y_true[idx][:k]
    return np.mean(top_k)

# Evaluation results
eval_results = {}

for col in pred_df.columns[:-2]:  # skip Consensus and Election_Flag
    y_true = pred_df[col].values  # binary labels (1=anomaly)
    y_scores = results[col][0]["scores"]  # anomaly scores from Step 4

    # Average Precision
    ap = average_precision_score(y_true, y_scores)

    # PR AUC
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    pr_auc = auc(recall, precision)

    # P@k for k = 8 (operational review size) and k = 4 (sensitivity test)
    p_at_8 = precision_at_k(y_true, y_scores, k=8)
    p_at_4 = precision_at_k(y_true, y_scores, k=4)

    eval_results[col] = {
        "Average Precision": ap,
        "PR AUC": pr_auc,
        "P@8": p_at_8,
        "P@4": p_at_4,
        "Num Anomalies": y_true.sum(),
        "Election Period Anomalies": y_true[pred_df['Election_Flag'] == 1].sum()
    }

# Include consensus
y_true = pred_df["Consensus_Anomaly"].values
scores = pred_df[["IsolationForest", "LOF", "KNN", "HBOS", "EllipticEnvelope"]].sum(axis=1)  # voting score
ap = average_precision_score(y_true, scores)
precision, recall, _ = precision_recall_curve(y_true, scores)
pr_auc = auc(recall, precision)
p_at_8 = precision_at_k(y_true, scores, k=8)
p_at_4 = precision_at_k(y_true, scores, k=4)

eval_results["Consensus"] = {
    "Average Precision": ap,
    "PR AUC": pr_auc,
    "P@8": p_at_8,
    "P@4": p_at_4,
    "Num Anomalies": y_true.sum(),
    "Election Period Anomalies": y_true[pred_df['Election_Flag'] == 1].sum()
}

# Display results
eval_df = pd.DataFrame(eval_results).T

# Optional visualizations
# 1. Scatter: VAT vs GDP Composite Index with anomalies highlighted
plt.figure(figsize=(8,6))
plt.scatter(df_clean['VAT'], df_clean['GDP Composite Index'],
            c=pred_df['Consensus_Anomaly'], cmap='coolwarm', alpha=0.7)
plt.xlabel('VAT')
plt.ylabel('GDP Composite Index')
plt.title('Consensus Anomalies Highlighted')
plt.colorbar(label='Anomaly (1=Yes)')
plt.show()

# 2. Time series plot with anomalies marked
plt.figure(figsize=(12,5))
plt.plot(df_clean.index, df_clean['VAT'], label='VAT')
plt.scatter(df_clean.index[pred_df['Consensus_Anomaly']==1],
            df_clean['VAT'][pred_df['Consensus_Anomaly']==1], color='red', label='Anomalies')
plt.xlabel('Time')
plt.ylabel('VAT')
plt.title('VAT Time Series with Consensus Anomalies')
plt.legend()
plt.show()


eval_df

"""## **Interpretation of Results**


**Model performance:**
Isolation Forest, Local Outlier Factor (LOF), and Elliptic Envelope each identified three anomalies and achieved perfect precision scores. Importantly, all three anomalies occurred during election periods, which suggests that fiscal irregularities often coincide with political cycles. In contrast, KNN and HBOS did not detect any anomalies and showed random performance, indicating they were not suitable for this dataset. The consensus model took a more conservative approach, flagging only one anomaly. Since this required agreement among at least three models, it represents the most reliable signal of abnormal behavior.

**Visual patterns:**
The scatter plot of VAT against the GDP Composite Index highlighted one clear outlier, marked in red, which matched the consensus anomaly. This point stands apart from the usual VAT–GDP relationship, showing that the detected anomaly reflects a genuine departure from expected economic patterns. The time series plot of VAT also revealed spikes around periods 50 and 53, which aligned with the anomalies detected by Isolation Forest, LOF, and Elliptic Envelope. The fact that these spikes occurred during election periods strengthens the link between anomalies and political activity.

**Overall meaning:**
The evidence shows that anomalies in VAT and GDP behavior are strongly tied to election periods. The single consensus anomaly provides the most trustworthy indication of irregularity, while the additional anomalies from Isolation Forest, LOF, and Elliptic Envelope suggest that other election-related distortions may also exist. This points to a strong likelihood that fiscal behavior changes strategically during elections, whether through policy shifts, reporting adjustments, or external shocks.

### **New Suggested Topic: *Comparing Isolation-Based, Density-Based, and Statistical Models for Anomaly Detection in Nigerian Tax and Economic Data with Election Flags***
"""